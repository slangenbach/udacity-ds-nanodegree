version: "3.7"

# create Zookeeper service to manage Kafka
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:5.2.2
    ports:
    - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: "2181"

# create first Kafka node
  kafka0:
    image: confluentinc/cp-kafka:5.2.2
    ports:
    - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 0
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: "INTERNAL://kafka0:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
    depends_on:
    - "zookeeper"

# create second Kafka node
  kafka1:
    image: confluentinc/cp-kafka:5.2.2
    ports:
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: "INTERNAL://kafka1:19093,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
    depends_on:
      - "zookeeper"

# create third Kafka node
  kafka2:
    image: confluentinc/cp-kafka:5.2.2
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: "INTERNAL://kafka2:19094,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
    depends_on:
      - "zookeeper"

# create Spark master
  spark_master:
    image: gettyimages/spark:2.4.1-hadoop-3.0
    command: bin/spark-class org.apache.spark.deploy.master.Master -h master
    hostname: master
    environment:
      MASTER: spark://master:7077
      SPARK_CONF_DIR: /conf
      SPARK_PUBLIC_DNS: localhost
    expose:
      - 7001
      - 7002
      - 7003
      - 7004
      - 7005
      - 7077
      - 6066
    ports:
      - "4040:4040"
      - "6066:6066"
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./conf/master:/conf
      - ./data:/tmp/data

# create first Spark worker
  spark_worker:
    image: gettyimages/spark:2.4.1-hadoop-3.0
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077
    hostname: worker
    environment:
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_PUBLIC_DNS: localhost
    links:
      - "spark_master:master"
    expose:
      - 7012
      - 7013
      - 7014
      - 7015
      - 8881
    ports:
      - "8081:8081"
    volumes:
      - ./conf/worker:/conf
      - ./data:/tmp/data
    depends_on:
      - "spark_master"

# create second Spark worker
  spark_worker2:
    image: gettyimages/spark:2.4.1-hadoop-3.0
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077
    hostname: worker2
    environment:
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_WORKER_PORT: 8882
      SPARK_WORKER_WEBUI_PORT: 8082
      SPARK_PUBLIC_DNS: localhost
    links:
      - "spark_master:master"
    expose:
      - 7012
      - 7013
      - 7014
      - 7015
      - 8882
    ports:
      - "8082:8082"
    volumes:
      - ./conf/worker:/conf
      - ./data:/tmp/data
    depends_on:
      - "spark_master"